{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отчет лабораторной работа № 1 \n",
    "## Часть 1 \n",
    "## Выполнение разведочного анализа больших данных с использованием фреймворка Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использован датасет: https://www.kaggle.com/datasets/omertarikyilmaz/istabul-traffic-2020-2024\n",
    "### Часть 1\n",
    "\n",
    "В данной части работы рассмотрены:\n",
    "* загрузка данных из HDFS;\n",
    "* базовые преобразования данных;\n",
    "* загрузка преобразованных данных в таблицу `Apache Airflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключим необходимые библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import (\n",
    "    regexp_replace,\n",
    "    regexp_extract_all,\n",
    "    col,\n",
    "    lit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем объект конфигурации для `Apache Spark`, указав необходимые параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_configuration() -> SparkConf:\n",
    "    \"\"\"\n",
    "    Создает и конфигурирует экземпляр SparkConf для приложения Spark.\n",
    "\n",
    "    Returns:\n",
    "        SparkConf: Настроенный экземпляр SparkConf.\n",
    "    \"\"\"\n",
    "    # Получаем имя пользователя\n",
    "    user_name = os.getenv(\"USER\")\n",
    "    \n",
    "    conf = SparkConf()\n",
    "    conf.setAppName(\"lab 1 Test\")\n",
    "    conf.setMaster(\"local[*]\")\n",
    "    # conf.set(\"spark.submit.deployMode\", \"client\")\n",
    "    conf.set(\"spark.executor.memory\", \"12g\")\n",
    "    conf.set(\"spark.executor.cores\", \"8\")\n",
    "    conf.set(\"spark.executor.instances\", \"1\")\n",
    "    conf.set(\"spark.driver.memory\", \"4g\")\n",
    "    conf.set(\"spark.driver.cores\", \"1\")\n",
    "    # conf.set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0\")\n",
    "    # conf.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    # conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    # conf.set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n",
    "    # conf.set(\"spark.sql.catalog.spark_catalog.warehouse\", f\"hdfs:///user/{user_name}/warehouse\")\n",
    "    # conf.set(\"spark.sql.catalog.spark_catalog.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\n",
    "\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём сам объект конфигурации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = create_spark_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём и выводим на экран сессию `Apache Spark`. В процессе создания сессии происходит подключение к кластеру `Apache Hadoop`, что может занять некоторое время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://562f149d2b9d:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>lab 1 Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbdd88afcd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим csv файл датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/istanbulTraffic2020-2024.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполняем датафрейм данными из файла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .load(path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводим фрагмент датафрейма на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+----------------+-------+-------------+-------------+-------------+------------------+\n",
      "|          DATE_TIME|        LATITUDE|       LONGITUDE|GEOHASH|MINIMUM_SPEED|MAXIMUM_SPEED|AVERAGE_SPEED|NUMBER_OF_VEHICLES|\n",
      "+-------------------+----------------+----------------+-------+-------------+-------------+-------------+------------------+\n",
      "|2021-10-01 00:00:00|40.9323120117188|29.3609619140625| sxkbu5|           52|           85|           67|                 7|\n",
      "|2021-10-01 00:00:00|41.1245727539063|28.1854248046875| sxk1cq|           33|           89|           68|                 5|\n",
      "|2021-10-01 00:00:00|41.1190795898438|28.9434814453125| sxk9ft|           48|          144|           72|               149|\n",
      "|2021-10-01 00:00:00|40.9707641601563|29.3170166015625| sxkc54|           77|          104|           84|                24|\n",
      "|2021-10-01 00:00:00|41.2509155273438|28.6798095703125| sxk6st|           45|          104|           73|                 8|\n",
      "|2021-10-01 00:00:00|40.9542846679688|29.0972900390625| sxk8yp|            1|           71|           29|                43|\n",
      "|2021-10-01 00:00:00|40.9707641601563|29.0643310546875| sxk9j6|            5|           52|           27|                20|\n",
      "|2021-10-01 00:00:00|41.0531616210938|29.1741943359375| sxk9xc|           84|          106|           90|                 4|\n",
      "|2021-10-01 00:00:00|40.9213256835938|29.1412353515625| sxk8z1|            7|           43|           29|                 9|\n",
      "|2021-10-01 00:00:00|40.8554077148438|29.2950439453125| sxkb6t|            4|          154|           58|               224|\n",
      "|2021-10-01 00:00:00|41.2014770507813|28.8555908203125| sxkd2s|           19|          152|           85|                82|\n",
      "|2021-10-01 00:00:00|41.0202026367188|28.8995361328125| sxk93e|           11|           97|           49|                28|\n",
      "|2021-10-01 00:00:00|40.9268188476563|29.3389892578125| sxkbgd|           28|          118|           64|                50|\n",
      "|2021-10-01 00:00:00|41.1575317382813|28.8885498046875| sxkd1k|           50|          106|           74|                13|\n",
      "|2021-10-01 00:00:00|41.1849975585938|28.8226318359375| sxk6rc|           49|          152|           86|               124|\n",
      "|2021-10-01 00:00:00|40.9817504882813|29.0753173828125| sxk9js|            2|           68|           32|                32|\n",
      "|2021-10-01 00:00:00|41.2014770507813|29.2181396484375| sxkf2u|           80|           81|           80|                 1|\n",
      "|2021-10-01 00:00:00|40.8334350585938|29.3280029296875| sxkb73|           21|          146|           70|               223|\n",
      "|2021-10-01 00:00:00|41.0421752929688|28.7677001953125| sxk3qx|            6|            6|            6|                 1|\n",
      "|2021-10-01 00:00:00|40.9268188476563|29.1632080078125| sxk8zd|           13|           56|           31|                12|\n",
      "+-------------------+----------------+----------------+-------+-------------+-------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что в целях сохранения ясности изложения и сокращения расчетного времени имеет смысл рассматривать не все солбцы датасета. Оставим следующие колонки, удалив остальные:\n",
    "\n",
    "| Название столбца  | Расшифровка |\n",
    "| -------------------| ------------- |\n",
    "| DATE_TIME          | Точная временная метка наблюдения  |\n",
    "| LATITUDE           | Географическая широта места измерения  |\n",
    "| LONGITUDE          | Географическая долгота места измерения |\n",
    "| GEOHASH            | Строковый код для географической области |\n",
    "| MINIMUM_SPEED      | Минимальная зарегистрированная скорость (км/ч) |\n",
    "| MAXIMUM_SPEED      | Максимальная зарегистрированная скорость (км/ч) |\n",
    "| AVERAGE_SPEED      | Средняя скорость всех обнаруженных транспортных средств (км/ч) |\n",
    "| NUMBER_OF_VEHICLES |  Количество обнаруженных уникальных транспортных средств |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем на экран метаданные датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE_TIME: string (nullable = true)\n",
      " |-- LATITUDE: string (nullable = true)\n",
      " |-- LONGITUDE: string (nullable = true)\n",
      " |-- GEOHASH: string (nullable = true)\n",
      " |-- MINIMUM_SPEED: string (nullable = true)\n",
      " |-- MAXIMUM_SPEED: string (nullable = true)\n",
      " |-- AVERAGE_SPEED: string (nullable = true)\n",
      " |-- NUMBER_OF_VEHICLES: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим датасет на нули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+-------+-------------+-------------+-------------+------------------+\n",
      "|DATE_TIME|LATITUDE|LONGITUDE|GEOHASH|MINIMUM_SPEED|MAXIMUM_SPEED|AVERAGE_SPEED|NUMBER_OF_VEHICLES|\n",
      "+---------+--------+---------+-------+-------------+-------------+-------------+------------------+\n",
      "+---------+--------+---------+-------+-------------+-------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = df.columns\n",
    "df_with_nulls_or_empty = df.filter(\n",
    "    \" OR \".join([f\"{col} IS NULL OR {col} = ''\" for col in columns_to_check])\n",
    ")\n",
    "df_with_nulls_or_empty.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что датасет не содержит нули."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним в нужный формат для быстрого чтения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"data/istanbulTraffic.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Останавливаем сессию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
